{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /home/loganmilandin/.local/lib/python3.9/site-packages (0.27.3)\n",
      "Requirement already satisfied: tqdm in /home/loganmilandin/.local/lib/python3.9/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/lib/python3/dist-packages (from openai) (2.22.0)\n",
      "Requirement already satisfied: aiohttp in /home/loganmilandin/.local/lib/python3.9/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/loganmilandin/.local/lib/python3.9/site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/loganmilandin/.local/lib/python3.9/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->openai) (19.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/loganmilandin/.local/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/loganmilandin/.local/lib/python3.9/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/loganmilandin/.local/lib/python3.9/site-packages (from aiohttp->openai) (3.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/loganmilandin/.local/lib/python3.9/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->openai) (2.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import urllib.request, json \n",
    "import string, re\n",
    "import random\n",
    "\n",
    "with open('API_key.txt', 'r') as file:\n",
    "    openai.api_key = file.readline().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"data/compositional_celebrities_subset.json\"\n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)[\"data\"]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(str(s)))))\n",
    "\n",
    "def extract_answer(generated):\n",
    "    if '\\n' not in generated:\n",
    "        last_line =  generated\n",
    "    else: \n",
    "        last_line = generated.split('\\n')[-1]\n",
    "\n",
    "    if ':' not in last_line:\n",
    "        after_colon = last_line\n",
    "    else:\n",
    "        after_colon = generated.split(':')[-1]\n",
    "    after_colon = after_colon.strip()\n",
    "    if not after_colon.strip():\n",
    "        return \"\"\n",
    "    return normalize_answer(after_colon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_prefix = \"Answer the question I ask in a format as close as possible to the following examples. State the final answer portion AS CONCISELY AS POSSIBLE.\\n\\n\"\n",
    "system_prompt = f\"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.\\nKnowledge cutoff: 2021-09\\nCurrent date: 2023-04-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_is_right_EM(prediction, real_answers):\n",
    "    # might be multiple acceptable answers (this appears rare from my initial scan)\n",
    "    assert type(real_answers) == list\n",
    "    # should be cleaned already, do it again just in case\n",
    "    clean_pred = normalize_answer(prediction)\n",
    "    is_right = False\n",
    "    for potential_answer in real_answers:\n",
    "        potential_answer_clean = normalize_answer(potential_answer)\n",
    "        if clean_pred == potential_answer_clean:\n",
    "            is_right = True\n",
    "    return is_right\n",
    "\n",
    "def compute_is_right_cover_EM(prediction, real_answers):\n",
    "    # might be multiple acceptable answers (this appears rare from my initial scan)\n",
    "    assert type(real_answers) == list\n",
    "    # should be cleaned already, do it again just in case\n",
    "    clean_pred = normalize_answer(prediction)\n",
    "    is_right = False\n",
    "    for potential_answer in real_answers:\n",
    "        potential_answer_clean = normalize_answer(potential_answer)\n",
    "        if potential_answer_clean in clean_pred:\n",
    "            is_right = True\n",
    "\n",
    "    return is_right\n",
    "\n",
    "def compute_is_right_GPT_opinion(question, prediction, real_answers):\n",
    "    prompt = f\"\"\"Your task is to compare a language model's response to a question with one or more acceptable ground truth answers to said question.\n",
    "VERY IMPORTANT: Numerical or ID-code answers MUST MATCH EXACTLY (up to differences in units). For other response types you should interpet responses for semantic similarity to ground truth.\"\"\"\n",
    "    prompt += f\"Here is the question that was asked: \\\"{question}\\\"\\n\"\n",
    "    prompt += f\"Here are the acceptable answer(s) for this question: [\\\"{real_answers[0]}\\\"\"\n",
    "    for answer in real_answers[1:]:\n",
    "        prompt += f\", \\\"{answer}\\\"\"\n",
    "    prompt += \"]\\n\"\n",
    "    prompt += f\"Here is the response of the model which you to are to evaluate: \\\"{prediction}\\\"\\n\"\n",
    "    prompt += \"\"\"Respond \\\"CORRECT\\\" if the model's response matches one of the ground truths and \\\"INCORRECT\\\" otherwise.\n",
    "If the model disagrees with the premise of the question or claims there's no answer, you should consider it to be INCORRECT.\n",
    "You may include reasoning after your response if needed, but your first word should be \\\"CORRECT\\\" or \\\"INCORRECT\\\".\n",
    "VERY IMPORTANT: You MUST IGNORE your own opinion on the question's validity or its answer. ONLY COMPARE THE MODEL'S RESPONSE WITH THE GIVEN GROUND TRUTHS.\"\"\"\n",
    "    # print(prompt)\n",
    "    # print(prompt)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        max_tokens=200,\n",
    "        stop='\\n\\n',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response['choices'][0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LM_full_question(prompt, sample, current_model, extraction=extract_answer, max_tokens=450, start='Answer:'):\n",
    "    question = sample['Question']\n",
    "    # cur_prompt = prompt_prefix + prompt + '\\n' + '\\n' + 'Question: ' + question + '\\n' + start\n",
    "    cur_prompt = prompt + '\\n' + '\\n' + 'Question: ' + question + '\\n' + start\n",
    "\n",
    "    if current_model in [\"gpt-3.5-turbo\", \"gpt-4\"]:\n",
    "        ans = openai.ChatCompletion.create(\n",
    "            model=current_model,\n",
    "            max_tokens=max_tokens,\n",
    "            stop='\\n\\n',\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": cur_prompt}\n",
    "            ]\n",
    "        )\n",
    "        response_text = ans['choices'][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        ans = openai.Completion.create(\n",
    "            model=current_model,\n",
    "            max_tokens=max_tokens,\n",
    "            stop='\\n\\n',\n",
    "            prompt=cur_prompt,\n",
    "            temperature=0\n",
    "        )\n",
    "        response_text = ans['choices'][0]['text']\n",
    "    failure_response = {\n",
    "        \"Question\": sample['Question'],\n",
    "        \"Answer\": sample['Answer'],\n",
    "        \"Prediction\": response_text,\n",
    "        \"Cleaned_pred\": \"\",\n",
    "        \"is_right_EM\": False,\n",
    "        \"is_right_CEM\": False\n",
    "    }\n",
    "    if response_text.strip() == '':\n",
    "        return failure_response\n",
    "    # print(\"not empty: \" + response_text.strip())\n",
    "    clean_ans = extraction(response_text)\n",
    "    # print(\"clean answer: \" + clean_ans)\n",
    "    if clean_ans.strip() == '':\n",
    "        return failure_response\n",
    "    is_right_EM = compute_is_right_EM(clean_ans, sample['Answer'])\n",
    "    is_right_CEM = compute_is_right_cover_EM(clean_ans, sample['Answer'])\n",
    "    is_right_GPT_opinion = compute_is_right_GPT_opinion(question, response_text, sample['Answer'])\n",
    "    jsonres = {\n",
    "        \"question\": sample['Question'],\n",
    "        \"prompt\": cur_prompt,\n",
    "        \"answer\": sample['Answer'],\n",
    "        \"returned\": response_text,\n",
    "        \"cleaned_pred\": clean_ans,\n",
    "        \"is_right_EM\": is_right_EM,\n",
    "        \"is_right_CEM\": is_right_CEM,\n",
    "        \"is_right_GPT_opinion\": is_right_GPT_opinion\n",
    "    }\n",
    "    return jsonres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LM_subquestions(prompt_1, prompt_2, sample, current_model, extraction=extract_answer, max_tokens=250, start='Answer:'):\n",
    "    prompt1_with_q = prompt_1 + '\\n' + '\\n' + 'Question: ' + sample['Q1'] + '\\n' + start\n",
    "    prompt2_with_q = prompt_2 + '\\n' + '\\n' + 'Question: ' + sample['Q2'] + '\\n' + start\n",
    "\n",
    "    if current_model in [\"gpt-3.5-turbo\", \"gpt-4\"]:\n",
    "        q1_response = openai.ChatCompletion.create(\n",
    "            model=current_model,\n",
    "            max_tokens=max_tokens,\n",
    "            stop=\"\\n\\n\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt1_with_q}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        ans1 = q1_response['choices'][0][\"message\"][\"content\"]\n",
    "        \n",
    "        q2_response = openai.ChatCompletion.create(\n",
    "            model=current_model,\n",
    "            max_tokens=max_tokens,\n",
    "            stop=\"\\n\\n\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\":  system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt2_with_q}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        ans2 = q2_response['choices'][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        q1_response = openai.Completion.create(\n",
    "            model=current_model,\n",
    "            max_tokens=max_tokens,\n",
    "            stop=\"\\n\\n\",\n",
    "            prompt=prompt1_with_q,\n",
    "            temperature=0\n",
    "        )\n",
    "        ans1 = q1_response['choices'][0]['text']\n",
    "\n",
    "        q2_response = openai.Completion.create(\n",
    "            model=current_model,\n",
    "            max_tokens=max_tokens,\n",
    "            stop=\"\\n\\n\",\n",
    "            prompt=prompt2_with_q,\n",
    "            temperature=0\n",
    "        )\n",
    "        ans2 = q2_response['choices'][0]['text']\n",
    "\n",
    "    q1_clean_ans = extraction(ans1)\n",
    "    q1_is_right_EM = compute_is_right_EM(q1_clean_ans, sample['A1'])\n",
    "    q1_is_right_CEM = compute_is_right_cover_EM(q1_clean_ans, sample['A1'])\n",
    "\n",
    "    q2_clean_ans = extraction(ans2)\n",
    "    q2_is_right_EM = compute_is_right_EM(q2_clean_ans, sample['A2'])\n",
    "    q2_is_right_CEM = compute_is_right_cover_EM(q2_clean_ans, sample['A2'])\n",
    "\n",
    "    jsonres = {\n",
    "        \"Q1\": sample['Q1'],\n",
    "        \"Q1_gt\": sample['A1'],\n",
    "        \"Q1_pred\": q1_clean_ans,\n",
    "        \"Q1_is_right_EM\": q1_is_right_EM,\n",
    "        \"Q1_is_right_CEM\": q1_is_right_CEM,\n",
    "        \"Q2\": sample['Q2'],\n",
    "        \"Q2_gt\": sample['A2'],\n",
    "        \"Q2_pred\": q2_clean_ans,\n",
    "        \"Q2_is_right_EM\": q2_is_right_EM,\n",
    "        \"Q2_is_right_CEM\": q2_is_right_CEM,\n",
    "        \"Q1_and_Q2_right_EM\": q1_is_right_EM and q2_is_right_EM,\n",
    "        \"Q1_and_Q2_right_CEM\": q1_is_right_CEM and q2_is_right_CEM,\n",
    "        \"Q1_prompt\": prompt1_with_q,\n",
    "        \"Q2_prompt\": prompt2_with_q,\n",
    "        'Q1_returned': ans1,\n",
    "        'Q2_returned': ans2\n",
    "    }\n",
    "\n",
    "    return jsonres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the combined dictionary from the JSON file\n",
    "with open(\"data/prompts.json\", \"r\") as json_file:\n",
    "    all_prompts = json.load(json_file)\n",
    "\n",
    "# Extract the individual dictionaries\n",
    "chain_of_thought_prompt_dict = all_prompts[\"chain_of_thought_prompt_dict\"]\n",
    "self_ask_prompt_dict = all_prompts[\"self_ask_prompt_dict\"]\n",
    "direct_answer_prompt_dict = all_prompts[\"direct_answer_prompt_dict\"]\n",
    "subquestion_1_prompt_dict = all_prompts[\"subquestion_1_prompt_dict\"]\n",
    "subquestion_2_prompt_dict = all_prompts[\"subquestion_2_prompt_dict\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "results = {\n",
    "    \"per_question_results\": [],\n",
    "    \"summary\": {\n",
    "        \"full_question_direct_answer_correct_EM\": 0,\n",
    "        \"full_question_direct_answer_correct_CEM\": 0,\n",
    "        \"full_question_chain_of_thought_correct_EM\": 0,\n",
    "        \"full_question_chain_of_thought_correct_CEM\": 0,\n",
    "        \"full_question_self_ask_correct_EM\": 0,\n",
    "        \"full_question_self_ask_correct_CEM\": 0,\n",
    "        \"both_subquestions_correct_EM\": 0,\n",
    "        \"both_subquestions_correct_CEM\": 0,\n",
    "        \"subquestions_wrong_when_full_question_direct_answer_correct_EM\": 0,\n",
    "        \"subquestions_wrong_when_full_question_direct_answer_correct_CEM\": 0,\n",
    "        \"subquestions_wrong_when_full_question_chain_of_thought_correct_EM\": 0,\n",
    "        \"subquestions_wrong_when_full_question_chain_of_thought_correct_CEM\": 0,\n",
    "        \"subquestions_wrong_when_full_question_self_ask_correct_EM\": 0,\n",
    "        \"subquestions_wrong_when_full_question_self_ask_correct_CEM\": 0,\n",
    "        \"full_question_direct_answer_wrong_when_subquestions_correct_EM\": 0,\n",
    "        \"full_question_direct_answer_wrong_when_subquestions_correct_CEM\": 0,\n",
    "        \"full_question_chain_of_thought_wrong_when_subquestions_correct_EM\": 0,\n",
    "        \"full_question_chain_of_thought_wrong_when_subquestions_correct_CEM\": 0,\n",
    "        \"full_question_self_ask_wrong_when_subquestions_correct_EM\": 0,\n",
    "        \"full_question_self_ask_wrong_when_subquestions_correct_CEM\": 0,\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your task is to compare a language model's response to a question with one or more acceptable ground truth answers to said question.\n",
      "VERY IMPORTANT: Numerical or ID-code answers MUST MATCH EXACTLY (up to differences in units). For other response types you should interpet responses for semantic similarity to ground truth.Here is the question that was asked: \"Who was the President of the United States when Kawhi Leonard was born?\"\n",
      "Here are the acceptable answer(s) for this question: [\"George H. W. Bush\"]\n",
      "Here is the response of the model which you to are to evaluate: \"Bill Clinton.\"\n",
      "Respond \"CORRECT\" if the model's response matches one of the ground truths and \"INCORRECT\" otherwise.\n",
      "If the model disagrees with the premise of the question or claims there's no answer, you should consider it to be INCORRECT.\n",
      "You may include reasoning after your response if needed, but your first word should be \"CORRECT\" or \"INCORRECT\".\n",
      "VERY IMPORTANT: You MUST IGNORE your own opinion on the question's validity or its answer. ONLY COMPARE THE MODEL'S RESPONSE WITH THE GIVEN GROUND TRUTHS.\n",
      "Your task is to compare a language model's response to a question with one or more acceptable ground truth answers to said question.\n",
      "VERY IMPORTANT: Numerical or ID-code answers MUST MATCH EXACTLY (up to differences in units). For other response types you should interpet responses for semantic similarity to ground truth.Here is the question that was asked: \"Who was the President of the United States when Kawhi Leonard was born?\"\n",
      "Here are the acceptable answer(s) for this question: [\"George H. W. Bush\"]\n",
      "Here is the response of the model which you to are to evaluate: \"The date of birth of Kawhi Leonard is June 29, 1991. The President of the United States on June 29, 1991 was George H.W. Bush.\n",
      "So the final answer (a US president) is: George H.W. Bush.\"\n",
      "Respond \"CORRECT\" if the model's response matches one of the ground truths and \"INCORRECT\" otherwise.\n",
      "If the model disagrees with the premise of the question or claims there's no answer, you should consider it to be INCORRECT.\n",
      "You may include reasoning after your response if needed, but your first word should be \"CORRECT\" or \"INCORRECT\".\n",
      "VERY IMPORTANT: You MUST IGNORE your own opinion on the question's validity or its answer. ONLY COMPARE THE MODEL'S RESPONSE WITH THE GIVEN GROUND TRUTHS.\n",
      "question: Who was the President of the United States when Kawhi Leonard was born?\n",
      "prompt: Question: Who was the President of the United States when Chadwick Boseman was born?\n",
      "Answer: The date of birth of Chadwick Boseman is November 29, 1976. The President of the United States on November 29, 1976 was Gerald Ford.\n",
      "So the final answer (a US president) is: Gerald Ford.\n",
      "\n",
      "Question: Who was the President of the United States when Charles Robert Redford Jr. was born?\n",
      "Answer: The date of birth of Charles Robert Redford Jr. is August 18, 1936. The President of the United States on August 18, 1936 was Franklin D. Roosevelt.\n",
      "So the final answer (a US president) is: Franklin D. Roosevelt.\n",
      "\n",
      "Question: Who was the President of the United States when Kawhi Leonard was born?\n",
      "Answer:\n",
      "answer: ['George H. W. Bush']\n",
      "returned: The date of birth of Kawhi Leonard is June 29, 1991. The President of the United States on June 29, 1991 was George H.W. Bush.\n",
      "So the final answer (a US president) is: George H.W. Bush.\n",
      "cleaned_pred: george hw bush\n",
      "is_right_EM: False\n",
      "is_right_CEM: False\n",
      "is_right_GPT_opinion: CORRECT\n",
      "Your task is to compare a language model's response to a question with one or more acceptable ground truth answers to said question.\n",
      "VERY IMPORTANT: Numerical or ID-code answers MUST MATCH EXACTLY (up to differences in units). For other response types you should interpet responses for semantic similarity to ground truth.Here is the question that was asked: \"Who was the President of the United States when Kawhi Leonard was born?\"\n",
      "Here are the acceptable answer(s) for this question: [\"George H. W. Bush\"]\n",
      "Here is the response of the model which you to are to evaluate: \"Follow up: What is the birthdate of Kawhi Leonard?\n",
      "Intermediate answer: June 29, 1991.\n",
      "Follow up: Who was the President of the United States on June 29, 1991?\n",
      "Intermediate answer: George H.W. Bush.\n",
      "So the final answer is: George H.W. Bush.\"\n",
      "Respond \"CORRECT\" if the model's response matches one of the ground truths and \"INCORRECT\" otherwise.\n",
      "If the model disagrees with the premise of the question or claims there's no answer, you should consider it to be INCORRECT.\n",
      "You may include reasoning after your response if needed, but your first word should be \"CORRECT\" or \"INCORRECT\".\n",
      "VERY IMPORTANT: You MUST IGNORE your own opinion on the question's validity or its answer. ONLY COMPARE THE MODEL'S RESPONSE WITH THE GIVEN GROUND TRUTHS.\n",
      "0\n",
      "Your task is to compare a language model's response to a question with one or more acceptable ground truth answers to said question.\n",
      "VERY IMPORTANT: Numerical or ID-code answers MUST MATCH EXACTLY (up to differences in units). For other response types you should interpet responses for semantic similarity to ground truth.Here is the question that was asked: \"Who won the Nobel Prize in Literature in the year Cobie Smulders was born?\"\n",
      "Here are the acceptable answer(s) for this question: [\"Gabriel García Márquez\"]\n",
      "Here is the response of the model which you to are to evaluate: \"Dario Fo\"\n",
      "Respond \"CORRECT\" if the model's response matches one of the ground truths and \"INCORRECT\" otherwise.\n",
      "If the model disagrees with the premise of the question or claims there's no answer, you should consider it to be INCORRECT.\n",
      "You may include reasoning after your response if needed, but your first word should be \"CORRECT\" or \"INCORRECT\".\n",
      "VERY IMPORTANT: You MUST IGNORE your own opinion on the question's validity or its answer. ONLY COMPARE THE MODEL'S RESPONSE WITH THE GIVEN GROUND TRUTHS.\n",
      "Your task is to compare a language model's response to a question with one or more acceptable ground truth answers to said question.\n",
      "VERY IMPORTANT: Numerical or ID-code answers MUST MATCH EXACTLY (up to differences in units). For other response types you should interpet responses for semantic similarity to ground truth.Here is the question that was asked: \"Who won the Nobel Prize in Literature in the year Cobie Smulders was born?\"\n",
      "Here are the acceptable answer(s) for this question: [\"Gabriel García Márquez\"]\n",
      "Here is the response of the model which you to are to evaluate: \"The year of birth of Cobie Smulders is 1982. The winner of the Nobel Prize in Literature in 1982 was Gabriel Garcia Marquez. \n",
      "So the final answer (a Nobel Prize winner) is: Gabriel Garcia Marquez.\"\n",
      "Respond \"CORRECT\" if the model's response matches one of the ground truths and \"INCORRECT\" otherwise.\n",
      "If the model disagrees with the premise of the question or claims there's no answer, you should consider it to be INCORRECT.\n",
      "You may include reasoning after your response if needed, but your first word should be \"CORRECT\" or \"INCORRECT\".\n",
      "VERY IMPORTANT: You MUST IGNORE your own opinion on the question's validity or its answer. ONLY COMPARE THE MODEL'S RESPONSE WITH THE GIVEN GROUND TRUTHS.\n",
      "question: Who won the Nobel Prize in Literature in the year Cobie Smulders was born?\n",
      "prompt: Question: Who won the Nobel Prize in Literature in the year Harry Kane was born?\n",
      "Answer: The year of birth of Harry Kane is 1993. The winner of the Nobel Prize in Literature in 1993 was Toni Morrison.\n",
      "So the final answer (a nobel prize winner) is: Toni Morrison.\n",
      "\n",
      "Question: Who won the Nobel Prize in Literature in the year Fran Drescher was born?\n",
      "Answer: The year of birth of Fran Drescher is 1957. The winner of the Nobel Prize in Literature in 1957 was Albert Camus.\n",
      "So the final answer (a nobel prize winner) is: Albert Camus.\n",
      "\n",
      "Question: Who won the Nobel Prize in Literature in the year Cobie Smulders was born?\n",
      "Answer:\n",
      "answer: ['Gabriel García Márquez']\n",
      "returned: The year of birth of Cobie Smulders is 1982. The winner of the Nobel Prize in Literature in 1982 was Gabriel Garcia Marquez. \n",
      "So the final answer (a Nobel Prize winner) is: Gabriel Garcia Marquez.\n",
      "cleaned_pred: gabriel garcia marquez\n",
      "is_right_EM: False\n",
      "is_right_CEM: False\n",
      "is_right_GPT_opinion: CORRECT\n",
      "Your task is to compare a language model's response to a question with one or more acceptable ground truth answers to said question.\n",
      "VERY IMPORTANT: Numerical or ID-code answers MUST MATCH EXACTLY (up to differences in units). For other response types you should interpet responses for semantic similarity to ground truth.Here is the question that was asked: \"Who won the Nobel Prize in Literature in the year Cobie Smulders was born?\"\n",
      "Here are the acceptable answer(s) for this question: [\"Gabriel García Márquez\"]\n",
      "Here is the response of the model which you to are to evaluate: \"Follow up: In what year was Cobie Smulders born?\n",
      "Intermediate answer: 1982.\n",
      "Follow up: Who won the Nobel Prize in Literature in 1982?\n",
      "Intermediate answer: Gabriel García Márquez.\n",
      "So the final answer is: Gabriel García Márquez.\"\n",
      "Respond \"CORRECT\" if the model's response matches one of the ground truths and \"INCORRECT\" otherwise.\n",
      "If the model disagrees with the premise of the question or claims there's no answer, you should consider it to be INCORRECT.\n",
      "You may include reasoning after your response if needed, but your first word should be \"CORRECT\" or \"INCORRECT\".\n",
      "VERY IMPORTANT: You MUST IGNORE your own opinion on the question's validity or its answer. ONLY COMPARE THE MODEL'S RESPONSE WITH THE GIVEN GROUND TRUTHS.\n",
      "1\n",
      "Your task is to compare a language model's response to a question with one or more acceptable ground truth answers to said question.\n",
      "VERY IMPORTANT: Numerical or ID-code answers MUST MATCH EXACTLY (up to differences in units). For other response types you should interpet responses for semantic similarity to ground truth.Here is the question that was asked: \"What is the 3166-1 numeric code for the birthplace of Suleiman The Magnificent?\"\n",
      "Here are the acceptable answer(s) for this question: [\"792\"]\n",
      "Here is the response of the model which you to are to evaluate: \"There is no 3166-1 numeric code for the birthplace of Suleiman The Magnificent, as the 3166-1 standard is used for country codes and Suleiman was born before the concept of modern countries existed. Suleiman was born in 1494 in the Ottoman Empire, which is the predecessor state to modern-day Turkey. However, Turkey's 3166-1 numeric code is 792.\"\n",
      "Respond \"CORRECT\" if the model's response matches one of the ground truths and \"INCORRECT\" otherwise.\n",
      "If the model disagrees with the premise of the question or claims there's no answer, you should consider it to be INCORRECT.\n",
      "You may include reasoning after your response if needed, but your first word should be \"CORRECT\" or \"INCORRECT\".\n",
      "VERY IMPORTANT: You MUST IGNORE your own opinion on the question's validity or its answer. ONLY COMPARE THE MODEL'S RESPONSE WITH THE GIVEN GROUND TRUTHS.\n",
      "Your task is to compare a language model's response to a question with one or more acceptable ground truth answers to said question.\n",
      "VERY IMPORTANT: Numerical or ID-code answers MUST MATCH EXACTLY (up to differences in units). For other response types you should interpet responses for semantic similarity to ground truth.Here is the question that was asked: \"What is the 3166-1 numeric code for the birthplace of Suleiman The Magnificent?\"\n",
      "Here are the acceptable answer(s) for this question: [\"792\"]\n",
      "Here is the response of the model which you to are to evaluate: \"Suleiman The Magnificent was born in the Ottoman Empire, which is no longer a recognized country. The ISO 3166-1 standard only lists codes for current and commonly recognized countries or territories. Therefore, there is no ISO 3166-1 numeric code for the birthplace of Suleiman The Magnificent.\"\n",
      "Respond \"CORRECT\" if the model's response matches one of the ground truths and \"INCORRECT\" otherwise.\n",
      "If the model disagrees with the premise of the question or claims there's no answer, you should consider it to be INCORRECT.\n",
      "You may include reasoning after your response if needed, but your first word should be \"CORRECT\" or \"INCORRECT\".\n",
      "VERY IMPORTANT: You MUST IGNORE your own opinion on the question's validity or its answer. ONLY COMPARE THE MODEL'S RESPONSE WITH THE GIVEN GROUND TRUTHS.\n",
      "question: What is the 3166-1 numeric code for the birthplace of Suleiman The Magnificent?\n",
      "prompt: Question: What is the 3166-1 numeric code for the birthplace of Benicio Del Toro?\n",
      "Answer: The birthplace (country) of Benicio Del Toro is Puerto Rico. The ISO 3166-1 numeric code of Puerto Rico is 630.\n",
      "So the final answer (an ISO 3166-1 numeric code) is: 630.\n",
      "\n",
      "Question: What is the 3166-1 numeric code for the birthplace of Daniel Cousin?\n",
      "Answer: The birthplace (country) of Daniel Cousin is Gabon. The ISO 3166-1 numeric code of Gabon is 266.\n",
      "So the final answer (an ISO 3166-1 numeric code) is: 266.\n",
      "\n",
      "Question: What is the 3166-1 numeric code for the birthplace of Suleiman The Magnificent?\n",
      "Answer:\n",
      "answer: ['792']\n",
      "returned: Suleiman The Magnificent was born in the Ottoman Empire, which is no longer a recognized country. The ISO 3166-1 standard only lists codes for current and commonly recognized countries or territories. Therefore, there is no ISO 3166-1 numeric code for the birthplace of Suleiman The Magnificent.\n",
      "cleaned_pred: suleiman magnificent was born in ottoman empire which is no longer recognized country iso 31661 standard only lists codes for current and commonly recognized countries or territories therefore there is no iso 31661 numeric code for birthplace of suleiman magnificent\n",
      "is_right_EM: False\n",
      "is_right_CEM: False\n",
      "is_right_GPT_opinion: CORRECT. \n",
      "Your task is to compare a language model's response to a question with one or more acceptable ground truth answers to said question.\n",
      "VERY IMPORTANT: Numerical or ID-code answers MUST MATCH EXACTLY (up to differences in units). For other response types you should interpet responses for semantic similarity to ground truth.Here is the question that was asked: \"What is the 3166-1 numeric code for the birthplace of Suleiman The Magnificent?\"\n",
      "Here are the acceptable answer(s) for this question: [\"792\"]\n",
      "Here is the response of the model which you to are to evaluate: \"Follow up: What is the birthplace (country only) of Suleiman The Magnificent?\n",
      "Intermediate answer: Ottoman Empire (His birthplace is now modern-day Turkey).\n",
      "Follow up: What is the 3166-1 numeric code for Turkey?\n",
      "Intermediate answer: 792.\n",
      "So the final answer is: 792.\"\n",
      "Respond \"CORRECT\" if the model's response matches one of the ground truths and \"INCORRECT\" otherwise.\n",
      "If the model disagrees with the premise of the question or claims there's no answer, you should consider it to be INCORRECT.\n",
      "You may include reasoning after your response if needed, but your first word should be \"CORRECT\" or \"INCORRECT\".\n",
      "VERY IMPORTANT: You MUST IGNORE your own opinion on the question's validity or its answer. ONLY COMPARE THE MODEL'S RESPONSE WITH THE GIVEN GROUND TRUTHS.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m subquestion_1_prompt \u001b[39m=\u001b[39m subquestion_1_prompt_dict[category]\n\u001b[1;32m     36\u001b[0m subquestion_2_prompt \u001b[39m=\u001b[39m subquestion_2_prompt_dict[category]\n\u001b[0;32m---> 37\u001b[0m subquestions_result \u001b[39m=\u001b[39m run_LM_subquestions(subquestion_1_prompt, subquestion_2_prompt, dp, model, extraction\u001b[39m=\u001b[39;49mextract_answer)\n\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m subquestions_result[\u001b[39m\"\u001b[39m\u001b[39mQ1_and_Q2_right_EM\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     40\u001b[0m     results[\u001b[39m\"\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mboth_subquestions_correct_EM\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[41], line 6\u001b[0m, in \u001b[0;36mrun_LM_subquestions\u001b[0;34m(prompt_1, prompt_2, sample, current_model, extraction, max_tokens, start)\u001b[0m\n\u001b[1;32m      3\u001b[0m prompt2_with_q \u001b[39m=\u001b[39m prompt_2 \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mQuestion: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m sample[\u001b[39m'\u001b[39m\u001b[39mQ2\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m start\n\u001b[1;32m      5\u001b[0m \u001b[39mif\u001b[39;00m current_model \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgpt-4\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m----> 6\u001b[0m     q1_response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      7\u001b[0m         model\u001b[39m=\u001b[39;49mcurrent_model,\n\u001b[1;32m      8\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[1;32m      9\u001b[0m         stop\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m         messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     11\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: system_prompt},\n\u001b[1;32m     12\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt1_with_q}\n\u001b[1;32m     13\u001b[0m         ],\n\u001b[1;32m     14\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     ans1 \u001b[39m=\u001b[39m q1_response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     18\u001b[0m     q2_response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mcreate(\n\u001b[1;32m     19\u001b[0m         model\u001b[39m=\u001b[39mcurrent_model,\n\u001b[1;32m     20\u001b[0m         max_tokens\u001b[39m=\u001b[39mmax_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m         temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     27\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/openai/api_requestor.py:216\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[1;32m    219\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    220\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    221\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    222\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    223\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[1;32m    226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/openai/api_requestor.py:516\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    514\u001b[0m     _thread_context\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m _make_session()\n\u001b[1;32m    515\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 516\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    517\u001b[0m         method,\n\u001b[1;32m    518\u001b[0m         abs_url,\n\u001b[1;32m    519\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    520\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    521\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    522\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    523\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    524\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    527\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/requests/sessions.py:533\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    529\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m: timeout,\n\u001b[1;32m    530\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m: allow_redirects,\n\u001b[1;32m    531\u001b[0m }\n\u001b[1;32m    532\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 533\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    535\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/requests/sessions.py:646\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    645\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 646\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    648\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    649\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/requests/adapters.py:439\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 439\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    440\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    441\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    442\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    443\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    444\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    445\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    446\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    447\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    448\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    449\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    450\u001b[0m         )\n\u001b[1;32m    452\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m'\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:665\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    664\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 665\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    666\u001b[0m     conn,\n\u001b[1;32m    667\u001b[0m     method,\n\u001b[1;32m    668\u001b[0m     url,\n\u001b[1;32m    669\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    670\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    671\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    672\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    673\u001b[0m )\n\u001b[1;32m    675\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:421\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    416\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    417\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    419\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    420\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    422\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    423\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:416\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    415\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    417\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    419\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    420\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    421\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1378\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.9/ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "while i < len(data):\n",
    "    dp = data[i]\n",
    "    try:\n",
    "        category = dp['category']\n",
    "\n",
    "        # Direct answer\n",
    "        direct_answer_prompt = direct_answer_prompt_dict[category]\n",
    "        direct_answer_result = run_LM_full_question(direct_answer_prompt, dp, model)\n",
    "        if (not direct_answer_result[\"is_right_CEM\"] and \"INCORRECT\" not in direct_answer_result[\"is_right_GPT_opinion\"]) \\\n",
    "            or (direct_answer_result[\"is_right_CEM\"] and \"INCORRECT\" in direct_answer_result[\"is_right_GPT_opinion\"]):\n",
    "            for key in direct_answer_result.keys():\n",
    "                print(f\"{key}: {direct_answer_result[key]}\")\n",
    "\n",
    "        # for key in direct_answer_result.keys():\n",
    "        #     print(f\"{key}: {direct_answer_result[key]}\")\n",
    "\n",
    "\n",
    "        # Chain of thought\n",
    "        chain_of_thought_prompt = chain_of_thought_prompt_dict[category]\n",
    "        chain_of_thought_result = run_LM_full_question(chain_of_thought_prompt, dp, model)\n",
    "        if (not chain_of_thought_result[\"is_right_CEM\"] and \"INCORRECT\" not in chain_of_thought_result[\"is_right_GPT_opinion\"]) or \\\n",
    "            (chain_of_thought_result[\"is_right_CEM\"] and \"INCORRECT\" in chain_of_thought_result[\"is_right_GPT_opinion\"]):\n",
    "            for key in chain_of_thought_result.keys():\n",
    "                print(f\"{key}: {chain_of_thought_result[key]}\")\n",
    "\n",
    "\n",
    "        # Self-ask\n",
    "        self_ask_prompt = self_ask_prompt_dict[category]\n",
    "        self_ask_result = run_LM_full_question(self_ask_prompt, dp, model, start='Are follow up questions needed here: Yes.\\n')\n",
    "        # print(self_ask_result[\"Prediction\"])\n",
    "        # print(self_ask_result[\"Answer\"])\n",
    "\n",
    "        # Subquestions\n",
    "        subquestion_1_prompt = subquestion_1_prompt_dict[category]\n",
    "        subquestion_2_prompt = subquestion_2_prompt_dict[category]\n",
    "        subquestions_result = run_LM_subquestions(subquestion_1_prompt, subquestion_2_prompt, dp, model, extraction=extract_answer)\n",
    "\n",
    "        if subquestions_result[\"Q1_and_Q2_right_EM\"]:\n",
    "            results[\"summary\"][\"both_subquestions_correct_EM\"] += 1\n",
    "\n",
    "            if not direct_answer_result[\"is_right_EM\"]:\n",
    "                results[\"summary\"][\"full_question_direct_answer_wrong_when_subquestions_correct_EM\"] += 1\n",
    "\n",
    "            if not chain_of_thought_result[\"is_right_EM\"]:\n",
    "                results[\"summary\"][\"full_question_chain_of_thought_wrong_when_subquestions_correct_EM\"] += 1\n",
    "\n",
    "            if not self_ask_result[\"is_right_EM\"]:\n",
    "                results[\"summary\"][\"full_question_self_ask_wrong_when_subquestions_correct_EM\"] += 1\n",
    "\n",
    "        if subquestions_result[\"Q1_and_Q2_right_CEM\"]:\n",
    "            results[\"summary\"][\"both_subquestions_correct_CEM\"] += 1\n",
    "\n",
    "            if not direct_answer_result[\"is_right_CEM\"]:\n",
    "                results[\"summary\"][\"full_question_direct_answer_wrong_when_subquestions_correct_CEM\"] += 1\n",
    "\n",
    "            if not chain_of_thought_result[\"is_right_CEM\"]:\n",
    "                results[\"summary\"][\"full_question_chain_of_thought_wrong_when_subquestions_correct_CEM\"] += 1\n",
    "\n",
    "            if not self_ask_result[\"is_right_CEM\"]:\n",
    "                results[\"summary\"][\"full_question_self_ask_wrong_when_subquestions_correct_CEM\"] += 1\n",
    "\n",
    "        if direct_answer_result[\"is_right_EM\"]:\n",
    "            results[\"summary\"][\"full_question_direct_answer_correct_EM\"] += 1\n",
    "            if not subquestions_result[\"Q1_and_Q2_right_EM\"]:\n",
    "                results[\"summary\"][\"subquestions_wrong_when_full_question_direct_answer_correct_EM\"] += 1\n",
    "\n",
    "        if direct_answer_result[\"is_right_CEM\"]:\n",
    "            results[\"summary\"][\"full_question_direct_answer_correct_CEM\"] += 1\n",
    "            if not subquestions_result[\"Q1_and_Q2_right_CEM\"]:\n",
    "                results[\"summary\"][\"subquestions_wrong_when_full_question_direct_answer_correct_CEM\"] += 1\n",
    "\n",
    "        if chain_of_thought_result[\"is_right_EM\"]:\n",
    "            results[\"summary\"][\"full_question_chain_of_thought_correct_EM\"] += 1\n",
    "            if not subquestions_result[\"Q1_and_Q2_right_EM\"]:\n",
    "                results[\"summary\"][\"subquestions_wrong_when_full_question_chain_of_thought_correct_EM\"] += 1\n",
    "\n",
    "        if chain_of_thought_result[\"is_right_CEM\"]:\n",
    "            results[\"summary\"][\"full_question_chain_of_thought_correct_CEM\"] += 1\n",
    "            if not subquestions_result[\"Q1_and_Q2_right_CEM\"]:\n",
    "                results[\"summary\"][\"subquestions_wrong_when_full_question_chain_of_thought_correct_CEM\"] += 1\n",
    "\n",
    "        if self_ask_result[\"is_right_EM\"]:\n",
    "            results[\"summary\"][\"full_question_self_ask_correct_EM\"] += 1\n",
    "            if not subquestions_result[\"Q1_and_Q2_right_EM\"]:\n",
    "                results[\"summary\"][\"subquestions_wrong_when_full_question_self_ask_correct_EM\"] += 1\n",
    "\n",
    "        if self_ask_result[\"is_right_CEM\"]:\n",
    "            results[\"summary\"][\"full_question_self_ask_correct_CEM\"] += 1\n",
    "            if not subquestions_result[\"Q1_and_Q2_right_CEM\"]:\n",
    "                results[\"summary\"][\"subquestions_wrong_when_full_question_self_ask_correct_CEM\"] += 1\n",
    "        results[\"per_question_results\"].append({\n",
    "            \"subquestions_result\": subquestions_result,\n",
    "            \"direct_answer_result\": direct_answer_result,\n",
    "            \"chain_of_thought_result\": chain_of_thought_result,\n",
    "            \"self_ask_result\": self_ask_result\n",
    "        })\n",
    "        print(i)\n",
    "        i += 1\n",
    "    except Exception as e:\n",
    "        print(\"error: \", e)\n",
    "\n",
    "with open(f'results/CC_results_{model}_default_sysprompt.json', 'w') as outfile:\n",
    "    json.dump(results, outfile, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Summary:\n",
      "\n",
      "Full question direct answer correct em : 368\n",
      "Full question direct answer correct cem : 378\n",
      "Full question chain of thought correct em : 631\n",
      "Full question chain of thought correct cem : 650\n",
      "Full question self ask correct em : 601\n",
      "Full question self ask correct cem : 623\n",
      "Both subquestions correct em : 626\n",
      "Both subquestions correct cem : 638\n",
      "Subquestions wrong when full question direct answer correct em : 36\n",
      "Subquestions wrong when full question direct answer correct cem : 36\n",
      "Subquestions wrong when full question chain of thought correct em : 67\n",
      "Subquestions wrong when full question chain of thought correct cem : 69\n",
      "Subquestions wrong when full question self ask correct em : 56\n",
      "Subquestions wrong when full question self ask correct cem : 62\n",
      "Full question direct answer wrong when subquestions correct em : 294\n",
      "Full question direct answer wrong when subquestions correct cem : 296\n",
      "Full question chain of thought wrong when subquestions correct em : 62\n",
      "Full question chain of thought wrong when subquestions correct cem : 57\n",
      "Full question self ask wrong when subquestions correct em : 81\n",
      "Full question self ask wrong when subquestions correct cem : 77\n"
     ]
    }
   ],
   "source": [
    "print(\"Results Summary:\\n\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key.replace('_', ' ').capitalize()} : {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results/CC_results_{model}.json', 'w') as outfile:\n",
    "    json.dump(results, outfile, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
