{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /home/loganmilandin/.local/lib/python3.9/site-packages (0.27.3)\n",
      "Requirement already satisfied: tqdm in /home/loganmilandin/.local/lib/python3.9/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/lib/python3/dist-packages (from openai) (2.22.0)\n",
      "Requirement already satisfied: aiohttp in /home/loganmilandin/.local/lib/python3.9/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/loganmilandin/.local/lib/python3.9/site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/loganmilandin/.local/lib/python3.9/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->openai) (19.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/loganmilandin/.local/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/loganmilandin/.local/lib/python3.9/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/loganmilandin/.local/lib/python3.9/site-packages (from aiohttp->openai) (3.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/loganmilandin/.local/lib/python3.9/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->openai) (2.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import urllib.request, json \n",
    "import string, re\n",
    "import random\n",
    "\n",
    "with open('ofir_API_key.txt', 'r') as file:\n",
    "    openai.api_key = file.readline().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"data/compositional_celebrities_subset.json\"\n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)[\"data\"]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(str(s)))))\n",
    "\n",
    "def extract_answer(generated):\n",
    "    if '\\n' not in generated:\n",
    "        last_line =  generated\n",
    "    else: \n",
    "        last_line = generated.split('\\n')[-1]\n",
    "\n",
    "    if ':' not in last_line:\n",
    "        after_colon = last_line\n",
    "    else:\n",
    "        after_colon = generated.split(':')[-1]\n",
    "    after_colon = after_colon.strip()\n",
    "    if not after_colon.strip():\n",
    "        return \"\"\n",
    "    return normalize_answer(after_colon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_is_right_EM(prediction, real_answers):\n",
    "    # might be multiple acceptable answers (this appears rare from my initial scan)\n",
    "    assert type(real_answers) == list\n",
    "    # should be cleaned already, do it again just in case\n",
    "    clean_pred = normalize_answer(prediction)\n",
    "    is_right = False\n",
    "    for potential_answer in real_answers:\n",
    "        potential_answer_clean = normalize_answer(potential_answer)\n",
    "        if clean_pred == potential_answer_clean:\n",
    "            is_right = True\n",
    "    return is_right\n",
    "\n",
    "def compute_is_right_cover_EM(prediction, real_answers):\n",
    "    # might be multiple acceptable answers (this appears rare from my initial scan)\n",
    "    assert type(real_answers) == list\n",
    "    # should be cleaned already, do it again just in case\n",
    "    clean_pred = normalize_answer(prediction)\n",
    "    is_right = False\n",
    "    for potential_answer in real_answers:\n",
    "        potential_answer_clean = normalize_answer(potential_answer)\n",
    "        if potential_answer_clean in clean_pred:\n",
    "            is_right = True\n",
    "\n",
    "    return is_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_prefix = \"Answer the question I ask in a format as close as possible to the following examples. State the final answer portion AS CONCISELY AS POSSIBLE.\\n\\n\"\n",
    "system_prompt = f\"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.\\nKnowledge cutoff: 2021-09\\nCurrent date: 2023-04-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LM_full_question(prompt, sample, current_model, extraction=extract_answer, max_tokens=450, start='Answer:'):\n",
    "    question = sample['Question']\n",
    "    # cur_prompt = prompt_prefix + prompt + '\\n' + '\\n' + 'Question: ' + question + '\\n' + start\n",
    "    cur_prompt = prompt + '\\n' + '\\n' + 'Question: ' + question + '\\n' + start\n",
    "\n",
    "    if current_model in [\"gpt-3.5-turbo\", \"gpt-4\"]:\n",
    "        ans = openai.ChatCompletion.create(\n",
    "            model=current_model,\n",
    "            max_tokens=max_tokens,\n",
    "            stop='\\n\\n',\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": cur_prompt}\n",
    "            ]\n",
    "        )\n",
    "        response_text = ans['choices'][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        ans = openai.Completion.create(\n",
    "            model=current_model,\n",
    "            max_tokens=max_tokens,\n",
    "            stop='\\n\\n',\n",
    "            prompt=cur_prompt,\n",
    "            temperature=0\n",
    "        )\n",
    "        response_text = ans['choices'][0]['text']\n",
    "    failure_response = {\n",
    "        \"Question\": sample['Question'],\n",
    "        \"Answer\": sample['Answer'],\n",
    "        \"Prediction\": response_text,\n",
    "        \"Cleaned_pred\": \"\",\n",
    "        \"is_right_EM\": False,\n",
    "        \"is_right_CEM\": False\n",
    "    }\n",
    "    if response_text.strip() == '':\n",
    "        return failure_response\n",
    "    # print(\"not empty: \" + response_text.strip())\n",
    "    clean_ans = extraction(response_text)\n",
    "    # print(\"clean answer: \" + clean_ans)\n",
    "    if clean_ans.strip() == '':\n",
    "        return failure_response\n",
    "    is_right_EM = compute_is_right_EM(clean_ans, sample['Answer'])\n",
    "    is_right_CEM = compute_is_right_cover_EM(clean_ans, sample['Answer'])\n",
    "    jsonres = {\n",
    "        \"question\": sample['Question'],\n",
    "        \"prompt\": cur_prompt,\n",
    "        \"answer\": sample['Answer'],\n",
    "        \"returned\": response_text,\n",
    "        \"cleaned_pred\": clean_ans,\n",
    "        \"is_right_EM\": is_right_EM,\n",
    "        \"is_right_CEM\": is_right_CEM\n",
    "    }\n",
    "    return jsonres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LM_subquestions(prompt_1, prompt_2, sample, current_model, extraction=extract_answer, max_tokens=250, start='Answer:'):\n",
    "    prompt1_with_q = prompt_1 + '\\n' + '\\n' + 'Question: ' + sample['Q1'] + '\\n' + start\n",
    "    prompt2_with_q = prompt_2 + '\\n' + '\\n' + 'Question: ' + sample['Q2'] + '\\n' + start\n",
    "\n",
    "    if current_model in [\"gpt-3.5-turbo\", \"gpt-4\"]:\n",
    "        q1_response = openai.ChatCompletion.create(\n",
    "            model=current_model,\n",
    "            max_tokens=max_tokens,\n",
    "            stop=\"\\n\\n\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt1_with_q}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        ans1 = q1_response['choices'][0][\"message\"][\"content\"]\n",
    "        \n",
    "        q2_response = openai.ChatCompletion.create(\n",
    "            model=current_model,\n",
    "            max_tokens=max_tokens,\n",
    "            stop=\"\\n\\n\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\":  system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt2_with_q}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        ans2 = q2_response['choices'][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        q1_response = openai.Completion.create(\n",
    "            model=current_model,\n",
    "            max_tokens=max_tokens,\n",
    "            stop=\"\\n\\n\",\n",
    "            prompt=prompt1_with_q,\n",
    "            temperature=0\n",
    "        )\n",
    "        ans1 = q1_response['choices'][0]['text']\n",
    "\n",
    "        q2_response = openai.Completion.create(\n",
    "            model=current_model,\n",
    "            max_tokens=max_tokens,\n",
    "            stop=\"\\n\\n\",\n",
    "            prompt=prompt2_with_q,\n",
    "            temperature=0\n",
    "        )\n",
    "        ans2 = q2_response['choices'][0]['text']\n",
    "\n",
    "    q1_clean_ans = extraction(ans1)\n",
    "    q1_is_right_EM = compute_is_right_EM(q1_clean_ans, sample['A1'])\n",
    "    q1_is_right_CEM = compute_is_right_cover_EM(q1_clean_ans, sample['A1'])\n",
    "\n",
    "    q2_clean_ans = extraction(ans2)\n",
    "    q2_is_right_EM = compute_is_right_EM(q2_clean_ans, sample['A2'])\n",
    "    q2_is_right_CEM = compute_is_right_cover_EM(q2_clean_ans, sample['A2'])\n",
    "\n",
    "    jsonres = {\n",
    "        \"Q1\": sample['Q1'],\n",
    "        \"Q1_gt\": sample['A1'],\n",
    "        \"Q1_pred\": q1_clean_ans,\n",
    "        \"Q1_is_right_EM\": q1_is_right_EM,\n",
    "        \"Q1_is_right_CEM\": q1_is_right_CEM,\n",
    "        \"Q2\": sample['Q2'],\n",
    "        \"Q2_gt\": sample['A2'],\n",
    "        \"Q2_pred\": q2_clean_ans,\n",
    "        \"Q2_is_right_EM\": q2_is_right_EM,\n",
    "        \"Q2_is_right_CEM\": q2_is_right_CEM,\n",
    "        \"Q1_and_Q2_right_EM\": q1_is_right_EM and q2_is_right_EM,\n",
    "        \"Q1_and_Q2_right_CEM\": q1_is_right_CEM and q2_is_right_CEM,\n",
    "        \"Q1_prompt\": prompt1_with_q,\n",
    "        \"Q2_prompt\": prompt2_with_q,\n",
    "        'Q1_returned': ans1,\n",
    "        'Q2_returned': ans2\n",
    "    }\n",
    "\n",
    "    return jsonres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the combined dictionary from the JSON file\n",
    "with open(\"data/prompts.json\", \"r\") as json_file:\n",
    "    all_prompts = json.load(json_file)\n",
    "\n",
    "# Extract the individual dictionaries\n",
    "chain_of_thought_prompt_dict = all_prompts[\"chain_of_thought_prompt_dict\"]\n",
    "self_ask_prompt_dict = all_prompts[\"self_ask_prompt_dict\"]\n",
    "direct_answer_prompt_dict = all_prompts[\"direct_answer_prompt_dict\"]\n",
    "subquestion_1_prompt_dict = all_prompts[\"subquestion_1_prompt_dict\"]\n",
    "subquestion_2_prompt_dict = all_prompts[\"subquestion_2_prompt_dict\"]\n",
    "\n",
    "\n",
    "# Now you can use the individual dictionaries in your script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "results = {\n",
    "    \"per_question_results\": [],\n",
    "    \"summary\": {\n",
    "        \"full_question_direct_answer_correct_EM\": 0,\n",
    "        \"full_question_direct_answer_correct_CEM\": 0,\n",
    "        \"full_question_chain_of_thought_correct_EM\": 0,\n",
    "        \"full_question_chain_of_thought_correct_CEM\": 0,\n",
    "        \"full_question_self_ask_correct_EM\": 0,\n",
    "        \"full_question_self_ask_correct_CEM\": 0,\n",
    "        \"both_subquestions_correct_EM\": 0,\n",
    "        \"both_subquestions_correct_CEM\": 0,\n",
    "        \"subquestions_wrong_when_full_question_direct_answer_correct_EM\": 0,\n",
    "        \"subquestions_wrong_when_full_question_direct_answer_correct_CEM\": 0,\n",
    "        \"subquestions_wrong_when_full_question_chain_of_thought_correct_EM\": 0,\n",
    "        \"subquestions_wrong_when_full_question_chain_of_thought_correct_CEM\": 0,\n",
    "        \"subquestions_wrong_when_full_question_self_ask_correct_EM\": 0,\n",
    "        \"subquestions_wrong_when_full_question_self_ask_correct_CEM\": 0,\n",
    "        \"full_question_direct_answer_wrong_when_subquestions_correct_EM\": 0,\n",
    "        \"full_question_direct_answer_wrong_when_subquestions_correct_CEM\": 0,\n",
    "        \"full_question_chain_of_thought_wrong_when_subquestions_correct_EM\": 0,\n",
    "        \"full_question_chain_of_thought_wrong_when_subquestions_correct_CEM\": 0,\n",
    "        \"full_question_self_ask_wrong_when_subquestions_correct_EM\": 0,\n",
    "        \"full_question_self_ask_wrong_when_subquestions_correct_CEM\": 0,\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "model = \"gpt-4\"\n",
    "while i < len(data):\n",
    "    dp = data[i]\n",
    "    try:\n",
    "        category = dp['category']\n",
    "\n",
    "        # Direct answer\n",
    "        direct_answer_prompt = direct_answer_prompt_dict[category]\n",
    "        direct_answer_result = run_LM_full_question(direct_answer_prompt, dp, model)\n",
    "\n",
    "\n",
    "        # Chain of thought\n",
    "        chain_of_thought_prompt = chain_of_thought_prompt_dict[category]\n",
    "        chain_of_thought_result = run_LM_full_question(chain_of_thought_prompt, dp, model)\n",
    "\n",
    "\n",
    "        # Self-ask\n",
    "        self_ask_prompt = self_ask_prompt_dict[category]\n",
    "        self_ask_result = run_LM_full_question(self_ask_prompt, dp, model, start='Are follow up questions needed here: Yes.\\n')\n",
    "        # print(self_ask_result[\"Prediction\"])\n",
    "        # print(self_ask_result[\"Answer\"])\n",
    "\n",
    "        # Subquestions\n",
    "        subquestion_1_prompt = subquestion_1_prompt_dict[category]\n",
    "        subquestion_2_prompt = subquestion_2_prompt_dict[category]\n",
    "        subquestions_result = run_LM_subquestions(subquestion_1_prompt, subquestion_2_prompt, dp, model, extraction=extract_answer)\n",
    "\n",
    "        if subquestions_result[\"Q1_and_Q2_right_EM\"]:\n",
    "            results[\"summary\"][\"both_subquestions_correct_EM\"] += 1\n",
    "\n",
    "            if not direct_answer_result[\"is_right_EM\"]:\n",
    "                results[\"summary\"][\"full_question_direct_answer_wrong_when_subquestions_correct_EM\"] += 1\n",
    "\n",
    "            if not chain_of_thought_result[\"is_right_EM\"]:\n",
    "                results[\"summary\"][\"full_question_chain_of_thought_wrong_when_subquestions_correct_EM\"] += 1\n",
    "\n",
    "            if not self_ask_result[\"is_right_EM\"]:\n",
    "                results[\"summary\"][\"full_question_self_ask_wrong_when_subquestions_correct_EM\"] += 1\n",
    "\n",
    "        if subquestions_result[\"Q1_and_Q2_right_CEM\"]:\n",
    "            results[\"summary\"][\"both_subquestions_correct_CEM\"] += 1\n",
    "\n",
    "            if not direct_answer_result[\"is_right_CEM\"]:\n",
    "                results[\"summary\"][\"full_question_direct_answer_wrong_when_subquestions_correct_CEM\"] += 1\n",
    "\n",
    "            if not chain_of_thought_result[\"is_right_CEM\"]:\n",
    "                results[\"summary\"][\"full_question_chain_of_thought_wrong_when_subquestions_correct_CEM\"] += 1\n",
    "\n",
    "            if not self_ask_result[\"is_right_CEM\"]:\n",
    "                results[\"summary\"][\"full_question_self_ask_wrong_when_subquestions_correct_CEM\"] += 1\n",
    "\n",
    "        if direct_answer_result[\"is_right_EM\"]:\n",
    "            results[\"summary\"][\"full_question_direct_answer_correct_EM\"] += 1\n",
    "            if not subquestions_result[\"Q1_and_Q2_right_EM\"]:\n",
    "                results[\"summary\"][\"subquestions_wrong_when_full_question_direct_answer_correct_EM\"] += 1\n",
    "\n",
    "        if direct_answer_result[\"is_right_CEM\"]:\n",
    "            results[\"summary\"][\"full_question_direct_answer_correct_CEM\"] += 1\n",
    "            if not subquestions_result[\"Q1_and_Q2_right_CEM\"]:\n",
    "                results[\"summary\"][\"subquestions_wrong_when_full_question_direct_answer_correct_CEM\"] += 1\n",
    "\n",
    "        if chain_of_thought_result[\"is_right_EM\"]:\n",
    "            results[\"summary\"][\"full_question_chain_of_thought_correct_EM\"] += 1\n",
    "            if not subquestions_result[\"Q1_and_Q2_right_EM\"]:\n",
    "                results[\"summary\"][\"subquestions_wrong_when_full_question_chain_of_thought_correct_EM\"] += 1\n",
    "\n",
    "        if chain_of_thought_result[\"is_right_CEM\"]:\n",
    "            results[\"summary\"][\"full_question_chain_of_thought_correct_CEM\"] += 1\n",
    "            if not subquestions_result[\"Q1_and_Q2_right_CEM\"]:\n",
    "                results[\"summary\"][\"subquestions_wrong_when_full_question_chain_of_thought_correct_CEM\"] += 1\n",
    "\n",
    "        if self_ask_result[\"is_right_EM\"]:\n",
    "            results[\"summary\"][\"full_question_self_ask_correct_EM\"] += 1\n",
    "            if not subquestions_result[\"Q1_and_Q2_right_EM\"]:\n",
    "                results[\"summary\"][\"subquestions_wrong_when_full_question_self_ask_correct_EM\"] += 1\n",
    "\n",
    "        if self_ask_result[\"is_right_CEM\"]:\n",
    "            results[\"summary\"][\"full_question_self_ask_correct_CEM\"] += 1\n",
    "            if not subquestions_result[\"Q1_and_Q2_right_CEM\"]:\n",
    "                results[\"summary\"][\"subquestions_wrong_when_full_question_self_ask_correct_CEM\"] += 1\n",
    "        results[\"per_question_results\"].append({\n",
    "            \"subquestions_result\": subquestions_result,\n",
    "            \"direct_answer_result\": direct_answer_result,\n",
    "            \"chain_of_thought_result\": chain_of_thought_result,\n",
    "            \"self_ask_result\": self_ask_result\n",
    "        })\n",
    "        print(i)\n",
    "        i += 1\n",
    "    except Exception as e:\n",
    "        print(\"error: \", e)\n",
    "\n",
    "with open(f'results/CC_results_{model}_default_sysprompt.json', 'w') as outfile:\n",
    "    json.dump(results, outfile, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Summary:\n",
      "\n",
      "Full question direct answer correct em : 368\n",
      "Full question direct answer correct cem : 378\n",
      "Full question chain of thought correct em : 631\n",
      "Full question chain of thought correct cem : 650\n",
      "Full question self ask correct em : 601\n",
      "Full question self ask correct cem : 623\n",
      "Both subquestions correct em : 626\n",
      "Both subquestions correct cem : 638\n",
      "Subquestions wrong when full question direct answer correct em : 36\n",
      "Subquestions wrong when full question direct answer correct cem : 36\n",
      "Subquestions wrong when full question chain of thought correct em : 67\n",
      "Subquestions wrong when full question chain of thought correct cem : 69\n",
      "Subquestions wrong when full question self ask correct em : 56\n",
      "Subquestions wrong when full question self ask correct cem : 62\n",
      "Full question direct answer wrong when subquestions correct em : 294\n",
      "Full question direct answer wrong when subquestions correct cem : 296\n",
      "Full question chain of thought wrong when subquestions correct em : 62\n",
      "Full question chain of thought wrong when subquestions correct cem : 57\n",
      "Full question self ask wrong when subquestions correct em : 81\n",
      "Full question self ask wrong when subquestions correct cem : 77\n"
     ]
    }
   ],
   "source": [
    "print(\"Results Summary:\\n\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key.replace('_', ' ').capitalize()} : {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results/CC_results_{model}.json', 'w') as outfile:\n",
    "    json.dump(results, outfile, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
