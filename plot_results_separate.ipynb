{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/CC_results_GPT-3.5-turbo.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Load data from JSON files\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models:\n\u001b[0;32m----> 7\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mresults/CC_results_\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel\u001b[39m}\u001b[39;49;00m\u001b[39m.json\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m infile:\n\u001b[1;32m      8\u001b[0m         data[model] \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(infile)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Calculate full question accuracy for each prompt type and model\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/CC_results_GPT-3.5-turbo.json'"
     ]
    }
   ],
   "source": [
    "models = ['ada', 'babbage', 'curie', 'GPT-3.5-turbo']\n",
    "models_to_short_names = {'ada': 'ada', 'babbage': 'babbage', 'curie': 'curie', 'text-davinci-003': 'davinci', \"davinci\": \"davinci\", 'GPT-3.5-turbo': '3.5-turbo'}\n",
    "data = {}\n",
    "\n",
    "# Load data from JSON files\n",
    "for model in models:\n",
    "    with open(f'results/CC_results_{model}.json', 'r') as infile:\n",
    "        data[model] = json.load(infile)\n",
    "\n",
    "# Calculate full question accuracy for each prompt type and model\n",
    "dataset_size = 1000\n",
    "direct_answer_accuracy_EM = [data[model]['full_question_direct_answer_correct_EM'] / dataset_size for model in models]\n",
    "chain_of_thought_accuracy_EM = [data[model]['full_question_chain_of_thought_correct_EM'] / dataset_size for model in models]\n",
    "self_ask_accuracy_EM = [data[model]['full_question_self_ask_correct_EM'] / dataset_size for model in models]\n",
    "subquestions_accuracy_EM = [data[model]['both_subquestions_correct_EM'] / dataset_size for model in models]\n",
    "\n",
    "direct_answer_accuracy_CEM = [data[model]['full_question_direct_answer_correct_CEM'] / dataset_size for model in models]\n",
    "chain_of_thought_accuracy_CEM = [data[model]['full_question_chain_of_thought_correct_CEM'] / dataset_size for model in models]\n",
    "self_ask_accuracy_CEM = [data[model]['full_question_self_ask_correct_CEM'] / dataset_size for model in models]\n",
    "subquestions_accuracy_CEM = [data[model]['both_subquestions_correct_CEM'] / dataset_size for model in models]\n",
    "\n",
    "# Bar plot settings\n",
    "bar_width = 0.35\n",
    "x = np.arange(len(models))\n",
    "\n",
    "# Function to create a bar plot for each prompt methodology\n",
    "def create_plot(title, full_question_accuracy, subquestions_accuracy, file_name):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.bar(x - bar_width / 2, full_question_accuracy, width=bar_width, label=f'Compositional Question Correct')\n",
    "    ax.bar(x + bar_width / 2, subquestions_accuracy, width=bar_width, label='Both Subquestions Correct')\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        comp_gap = (subquestions_accuracy[i] - full_question_accuracy[i]) / subquestions_accuracy[i] * 100\n",
    "        y_start = full_question_accuracy[i]\n",
    "        y_end = subquestions_accuracy[i]\n",
    "        x_pos = x[i] - bar_width / 2\n",
    "        ax.vlines(x_pos, y_start, y_end, linestyle='dotted', color='black')\n",
    "        print(comp_gap)\n",
    "        ax.annotate(f'{comp_gap:.1f}%', xy=(x_pos, (y_start + y_end) / 2), fontsize=12, color='black', ha='right')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([models_to_short_names[model] for model in models])\n",
    "    \n",
    "    # Move the legend below the plot\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_name)\n",
    "    plt.show()\n",
    "\n",
    "# Create and save plots for each prompt methodology\n",
    "create_plot('Compositionality Gap with Direct Answer Prompt', direct_answer_accuracy_EM, subquestions_accuracy_EM, 'direct_answer_EM_plot.png')\n",
    "create_plot('Chain-of-thought', chain_of_thought_accuracy_EM, subquestions_accuracy_EM, 'chain_of_thought_EM_plot.png')\n",
    "create_plot('Self-ask', self_ask_accuracy_EM, subquestions_accuracy_EM, 'self_ask_EM_plot_.png')\n",
    "\n",
    "create_plot('Compositionality Gap with Direct Answer Prompt', direct_answer_accuracy_EM, subquestions_accuracy_EM, 'direct_answer_CEM_plot.png')\n",
    "create_plot('Chain-of-thought', chain_of_thought_accuracy_EM, subquestions_accuracy_EM, 'chain_of_thought_CEM_plot.png')\n",
    "create_plot('Self-ask', self_ask_accuracy_EM, subquestions_accuracy_EM, 'self_ask_CEM_plot_.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
