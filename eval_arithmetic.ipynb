{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import urllib.request, json \n",
    "import string, re\n",
    "import random\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "\n",
    "with open('API_key.txt', 'r') as file:\n",
    "    openai.api_key = file.readline().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"data/all_formats_small_numbers.json\"\n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(str(s)))))\n",
    "\n",
    "def extract_answer(generated):\n",
    "    generated = generated.lower()\n",
    "    if 'final answer:' in generated:\n",
    "        after_colon = generated.split('final answer:')[-1]\n",
    "        if \"\\n\" in after_colon:\n",
    "            after_colon = after_colon.split(\"\\n\")[0]\n",
    "    elif \":\" in generated:\n",
    "        after_colon = generated.split(':')[-1]\n",
    "        if \"\\n\" in after_colon:\n",
    "            after_colon = after_colon.split(\"\\n\")[0]\n",
    "    else:\n",
    "        after_colon = generated\n",
    "    return normalize_answer(after_colon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.\\nKnowledge cutoff: 2021-09\\nCurrent date: 2023-04-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LM_on_expression(prompt, sample, current_model, extract_answer, max_tokens=300, start=\" =\"):\n",
    "    question = sample['Question'] + start\n",
    "    # cur_prompt = prompt_prefix + prompt + '\\n' + '\\n' + 'Question: ' + question + '\\n' + start\n",
    "    cur_prompt = prompt + ' ' + question\n",
    "\n",
    "    if current_model in [\"gpt-3.5-turbo\", \"gpt-4\"]:\n",
    "        ans = openai.ChatCompletion.create(\n",
    "            model=current_model,\n",
    "            max_tokens=max_tokens,\n",
    "            # stop='\\n\\n',\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": cur_prompt}\n",
    "            ]\n",
    "        )\n",
    "        response_text = ans['choices'][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        ans = openai.Completion.create(\n",
    "            model=current_model,\n",
    "            max_tokens=max_tokens,\n",
    "            # stop='\\n\\n',\n",
    "            prompt=cur_prompt,\n",
    "            temperature=0\n",
    "        )\n",
    "        response_text = ans['choices'][0]['text']\n",
    "    is_right = False\n",
    "    try:\n",
    "        if prompt == \"\":\n",
    "            is_right = str(sample[\"Answer\"]) in normalize_answer(response_text)\n",
    "        else:\n",
    "            cleaned_answer = extract_answer(response_text)\n",
    "            is_right = int(cleaned_answer) == sample[\"Answer\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(cur_prompt)\n",
    "        print(response_text)\n",
    "        print(e)\n",
    "    jsonres = {\n",
    "        \"question\": sample['Question'],\n",
    "        \"prompt\": cur_prompt,\n",
    "        \"answer\": sample['Answer'],\n",
    "        \"returned\": response_text,\n",
    "        \"is_right\": is_right,\n",
    "        \"is_right_CEM\": str(sample[\"Answer\"]) in normalize_answer(response_text)\n",
    "    }\n",
    "    return jsonres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_results_dict(results, prompt_results, prompt_types):\n",
    "\n",
    "    for key in prompt_types:\n",
    "        if prompt_results[key][f\"is_right\"]:\n",
    "            results[\"summary\"][f\"{key}_correct\"] += 1\n",
    "        if prompt_results[key][f\"is_right_CEM\"]:\n",
    "            results[\"summary\"][f\"{key}_CEM_correct\"] += 1\n",
    "\n",
    "    results[\"per_question_results\"].append(prompt_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# + # + #', '# + # + # + #', '# + # + # + # + #', '# + # + # + # + # + #']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'per_question_results': [], 'summary': {'direct_answer_correct': 0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formats = [' + '.join(['#'] * (i+2)) for i in range(1, 5)] #+ [' + '.join(['#'] * (i+2)) for i in range(1, 5)]\n",
    "# formats = [\"# + # + #\", \"# * # * #\"]\n",
    "\n",
    "prompts = {\n",
    "    # \"unprompted\": \"\", \n",
    "    # \"reasoning_ok\": \"Answer the following math problem, formatting your final answer as \\\"final answer: <number>\\\". You may show your work. \", \n",
    "    \"direct_answer\": \"Answer the following math problem, final answer (number) only, NO WORDS. \"\n",
    "}\n",
    "results = {\n",
    "    f\"results_format_({format})\": {\n",
    "        \"per_question_results\": [],\n",
    "        \"summary\": {\n",
    "            **{f\"{prompt_type}_correct\": 0 for prompt_type in prompts.keys()}, **{f\"{prompt_type}_CEM_correct\": 0 for prompt_type in prompts.keys()}\n",
    "        }\n",
    "    } for format in formats\n",
    "}\n",
    "\n",
    "print(formats)\n",
    "\n",
    "{\n",
    "    \"per_question_results\": [],\n",
    "    \"summary\": {\n",
    "        \"direct_answer_correct\": 0,\n",
    "        # \"unprompted_correct\": 0\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_path = \"data/add_mult_only_3.json\"\n",
    "with open(wp_path, 'r') as f:\n",
    "    wps = json.load(f)\n",
    "len(wps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 449, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 444, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/http/client.py\", line 1344, in getresponse\n",
      "    response.begin()\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/http/client.py\", line 307, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/http/client.py\", line 268, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/requests/adapters.py\", line 487, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/urllib3/util/retry.py\", line 550, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/urllib3/packages/six.py\", line 770, in reraise\n",
      "    raise value\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 451, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 340, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_requestor.py\", line 520, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/requests/sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/requests/sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/requests/adapters.py\", line 533, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_24037/3247248736.py\", line 17, in <module>\n",
      "    prompt_results[key] = run_LM_on_expression(prompt, dp, model, extract_answer=extract_answer, start=\"\")\n",
      "  File \"/tmp/ipykernel_24037/1293680870.py\", line 7, in run_LM_on_expression\n",
      "    ans = openai.ChatCompletion.create(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_requestor.py\", line 220, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_requestor.py\", line 531, in request_raw\n",
      "    raise error.Timeout(\"Request timed out: {}\".format(e)) from e\n",
      "openai.error.Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_24037/3247248736.py\", line 17, in <module>\n",
      "    prompt_results[key] = run_LM_on_expression(prompt, dp, model, extract_answer=extract_answer, start=\"\")\n",
      "  File \"/tmp/ipykernel_24037/1293680870.py\", line 7, in run_LM_on_expression\n",
      "    ans = openai.ChatCompletion.create(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b7a9b2f60fd34b0f9e87a8b76b32cfc8 in your message.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_24037/3247248736.py\", line 17, in <module>\n",
      "    prompt_results[key] = run_LM_on_expression(prompt, dp, model, extract_answer=extract_answer, start=\"\")\n",
      "  File \"/tmp/ipykernel_24037/1293680870.py\", line 7, in run_LM_on_expression\n",
      "    ans = openai.ChatCompletion.create(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 52e021e5342f416cfc57330e26df3e3d in your message.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_24037/3247248736.py\", line 17, in <module>\n",
      "    prompt_results[key] = run_LM_on_expression(prompt, dp, model, extract_answer=extract_answer, start=\"\")\n",
      "  File \"/tmp/ipykernel_24037/1293680870.py\", line 7, in run_LM_on_expression\n",
      "    ans = openai.ChatCompletion.create(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 98eabc36f74cb929a6a1bf33ca46a80a in your message.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_24037/3247248736.py\", line 17, in <module>\n",
      "    prompt_results[key] = run_LM_on_expression(prompt, dp, model, extract_answer=extract_answer, start=\"\")\n",
      "  File \"/tmp/ipykernel_24037/1293680870.py\", line 7, in run_LM_on_expression\n",
      "    ans = openai.ChatCompletion.create(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6193864bf41778cbaffdcbffed6f1c40 in your message.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_24037/3247248736.py\", line 17, in <module>\n",
      "    prompt_results[key] = run_LM_on_expression(prompt, dp, model, extract_answer=extract_answer, start=\"\")\n",
      "  File \"/tmp/ipykernel_24037/1293680870.py\", line 7, in run_LM_on_expression\n",
      "    ans = openai.ChatCompletion.create(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_requestor.py\", line 230, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_requestor.py\", line 624, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/rjha01/anaconda3/envs/cse446/lib/python3.8/site-packages/openai/api_requestor.py\", line 687, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ef370327d49717cc9c2be029dca802ab in your message.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Answer the following math problem, final answer (number) only, NO WORDS.  A marathon runner ran 15 miles on Monday, 19 miles on Tuesday, 4 miles on Wednesday, 2 miles on Thursday, and 0 miles on Friday. How many miles did the runner cover in total?\n",
      "The runner covered 40 miles in total.\n",
      "invalid literal for int() with base 10: 'runner covered 40 miles in total'\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "for format in formats:\n",
    "    i = 0\n",
    "\n",
    "    while i < len(data[format]):\n",
    "        dp = data[format][i].copy()\n",
    "        ns = re.split(r'\\s\\*\\s|\\s\\+\\s', dp['Question'])\n",
    "        q = wps[format][i % 5]\n",
    "        for j in range(len(ns)):\n",
    "            idx = len(ns) - j\n",
    "            q = q.replace('#' * idx, ns[idx - 1])\n",
    "            \n",
    "        dp['Question'] = q\n",
    "        try:\n",
    "            prompt_results = {}\n",
    "            for key in prompts.keys():\n",
    "                prompt = prompts[key]\n",
    "                prompt_results[key] = run_LM_on_expression(prompt, dp, model, extract_answer=extract_answer, start=\"\")\n",
    "            # Update results\n",
    "            update_results_dict(results[f\"results_format_({format})\"], prompt_results, prompts.keys())\n",
    "\n",
    "            # only run once\n",
    "            print(i)\n",
    "            i += 1\n",
    "        except Exception as e:\n",
    "            # print(\"error: \", e)\n",
    "            traceback.print_exc()\n",
    "    # save()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/arithmetic/word_problem_only_results_mult_gpt-3.5-turbo.json'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_file = f'results/arithmetic/word_problem_only_results_add_{model}.json'\n",
    "results_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save():\n",
    "    with open(results_file, 'w') as outfile:\n",
    "        json.dump(results, outfile, ensure_ascii=False, indent=4)\n",
    "save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('cse446')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "636ec8becea98e7bcc147f8730e623b9daacd3a9f051eaf5633e292a90ea16aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
